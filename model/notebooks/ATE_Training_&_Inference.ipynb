{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinscaria/InstructABSA/blob/main/ATE_Training_%26_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2xc1EQjq3LY"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4oIyN4lu6qm",
        "outputId": "d587b6c0-2d8f-4f8e-f5f3-e15767d6d8e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount = True)\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTZzGWQTQAxt"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "  !pip install transformers\n",
        "  !pip install datasets\n",
        "  !pip install evaluate\n",
        "  !pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oTPPfF2q5S5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "if IN_COLAB:\n",
        "    root_path = 'Enter drive path'\n",
        "else:\n",
        "    root_path = 'Enter local path'\n",
        "    \n",
        "use_mps = True if torch.has_mps else False\n",
        "os.chdir(root_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DvLkxG9lp2t"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "\n",
        "from InstructABSA.data_prep import DatasetLoader\n",
        "from InstructABSA.utils import T5Generator, T5Classifier\n",
        "from instructions import InstructionsHandler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ht-x1qxn_AxG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knn9pcJMXovr"
      },
      "outputs": [],
      "source": [
        "task_name = 'ate'\n",
        "experiment_name = 'lapt2014_iabsa1'\n",
        "model_checkpoint = 'allenai/tk-instruct-base-def-pos'\n",
        "print('Experiment Name: ', experiment_name)\n",
        "model_out_path = './Models'\n",
        "model_out_path = os.path.join(model_out_path, task_name, f\"{model_checkpoint.replace('/', '')}-{experiment_name}\")\n",
        "print('Model output path: ', model_out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hfAuLAvVe8D"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "id_train_file_path = './Dataset/SemEval14/Train/Laptops_Train.csv'\n",
        "id_test_file_path = './Dataset/SemEval14/Test/Laptops_Test.csv'\n",
        "id_tr_df = pd.read_csv(id_train_file_path)\n",
        "id_te_df = pd.read_csv(id_test_file_path)\n",
        "\n",
        "# Get the input text into the required format using Instructions\n",
        "instruct_handler = InstructionsHandler()\n",
        "\n",
        "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
        "instruct_handler.load_instruction_set1()\n",
        "\n",
        "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
        "loader = DatasetLoader(id_tr_df, id_te_df)\n",
        "if loader.train_df_id is not None:\n",
        "    loader.train_df_id = loader.create_data_in_ate_format(loader.train_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])\n",
        "if loader.test_df_id is not None:\n",
        "    loader.test_df_id = loader.create_data_in_ate_format(loader.test_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4v8QMMT4B7t"
      },
      "outputs": [],
      "source": [
        "# Create T5 utils object\n",
        "t5_exp = T5Generator(model_checkpoint)\n",
        "\n",
        "# Tokenize Dataset\n",
        "id_ds, id_tokenized_ds, ood_ds, ood_tokenized_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
        "\n",
        "# Training arguments\n",
        "training_args = {\n",
        "    'output_dir':model_out_path,\n",
        "    'evaluation_strategy':\"epoch\",\n",
        "    'learning_rate':5e-5,\n",
        "    'lr_scheduler_type':'cosine',\n",
        "    'per_device_train_batch_size':8,\n",
        "    'per_device_eval_batch_size':16,\n",
        "    'num_train_epochs':4,\n",
        "    'weight_decay':0.01,\n",
        "    'warmup_ratio':0.1,\n",
        "    'save_strategy':'no',\n",
        "    'load_best_model_at_end':False,\n",
        "    'push_to_hub':False,\n",
        "    'eval_accumulation_steps':1,\n",
        "    'predict_with_generate':True,\n",
        "    'use_mps_device':use_mps\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZr7BAy6eJOA"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "model_trainer = t5_exp.train(id_tokenized_ds, **training_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "7rqL4maz_Dlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "id_train_file_path = './Dataset/SemEval14/Train/Laptops_Train.csv'\n",
        "id_test_file_path = './Dataset/SemEval14/Test/Laptops_Test.csv'\n",
        "id_tr_df = pd.read_csv(id_train_file_path)\n",
        "id_te_df = pd.read_csv(id_test_file_path)\n",
        "\n",
        "# Get the input text into the required format using Instructions\n",
        "instruct_handler = InstructionsHandler()\n",
        "\n",
        "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
        "instruct_handler.load_instruction_set1()\n",
        "\n",
        "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
        "loader = DatasetLoader(id_tr_df, id_te_df)\n",
        "if loader.train_df_id is not None:\n",
        "    loader.train_df_id = loader.create_data_in_ate_format(loader.train_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])\n",
        "if loader.test_df_id is not None:\n",
        "    loader.test_df_id = loader.create_data_in_ate_format(loader.test_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])"
      ],
      "metadata": {
        "id": "Cy6aOHv4_FUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model inference - Loading from Checkpoint\n",
        "t5_exp = T5Generator(model_out_path)\n",
        "\n",
        "# Tokenize Datasets\n",
        "id_ds, id_tokenized_ds, ood_ds, ood_tokenzed_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
        "\n",
        "# Get prediction labels - Training set   \n",
        "id_tr_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'train', trained_model_path = model_out_path, batch_size = 16)\n",
        "id_tr_labels = [i.strip() for i in id_ds['train']['labels']]\n",
        "\n",
        "# Get prediction labels - Testing set\n",
        "id_te_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'test', trained_model_path = model_out_path, batch_size = 16)\n",
        "id_te_labels = [i.strip() for i in id_ds['test']['labels']]"
      ],
      "metadata": {
        "id": "kbcnbLXtt9Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p, r, f1 = t5_exp.get_metrics(id_tr_labels, id_tr_pred_labels)\n",
        "print('Train Precision: ', p)\n",
        "print('Train Recall: ', r)\n",
        "print('Train F1: ', f1)\n",
        "\n",
        "p, r, f1 = t5_exp.get_metrics(id_te_labels, id_te_pred_labels)\n",
        "print('Test Precision: ', p)\n",
        "print('Test Recall: ', r)\n",
        "print('Test F1: ', f1)"
      ],
      "metadata": {
        "id": "pBCUT9jDt8-Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "basegpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "070d57aa6b4a039a680ca3535d2f37da5ed020b02d8ccf58fedcd4e32b3636b0"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}